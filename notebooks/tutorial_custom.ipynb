{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be436aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 20:47:07.106225: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-13 20:47:07.899044: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-05-13 20:47:15.898112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-13 20:47:15.900975: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-13 20:47:15.902240: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import bernn\n",
    "from bernn import TrainAEThenClassifierHoldout\n",
    "from ax.service.managed_loop import optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8101f33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.7'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bernn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe0a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal example of using Ax to optimize the hyperparameters of a model\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.csv_file = 'adenocarcinoma_data.csv'\n",
    "        self.path = '../data'\n",
    "        self.dloss = 'inverseTriplet'\n",
    "        self.n_epochs = 10\n",
    "\n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a87f4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f952a638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 05-13 21:01:57] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter nu. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 21:01:57] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter lr. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 21:01:57] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter wd. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 21:01:57] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter smoothing. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 21:01:57] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter margin. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 21:01:57] ax.service.utils.instantiation: Inferred value type of ParameterType.INT for parameter warmup. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 21:01:57] ax.service.utils.instantiation: Inferred value type of ParameterType.INT for parameter disc_b_warmup. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 21:01:57] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter dropout. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 21:01:57] ax.service.utils.instantiation: Inferred value type of ParameterType.STRING for parameter scaler. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 21:01:57] ax.service.utils.instantiation: Inferred value type of ParameterType.INT for parameter layer2. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 21:01:57] ax.service.utils.instantiation: Inferred value type of ParameterType.INT for parameter layer1. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 21:01:57] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter gamma. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 21:01:57] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter reg_entropy. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 21:01:57] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter l1. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 21:01:57] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter prune_threshold. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 21:01:57] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='nu', parameter_type=FLOAT, range=[0.0001, 100.0]), RangeParameter(name='lr', parameter_type=FLOAT, range=[0.0001, 0.01], log_scale=True), RangeParameter(name='wd', parameter_type=FLOAT, range=[1e-08, 1e-05], log_scale=True), RangeParameter(name='smoothing', parameter_type=FLOAT, range=[0.0, 0.2]), RangeParameter(name='margin', parameter_type=FLOAT, range=[0.0, 10.0]), RangeParameter(name='warmup', parameter_type=INT, range=[1, 1000]), RangeParameter(name='disc_b_warmup', parameter_type=INT, range=[1, 2]), RangeParameter(name='dropout', parameter_type=FLOAT, range=[0.0, 0.5]), ChoiceParameter(name='scaler', parameter_type=STRING, values=['standard_per_batch', 'standard', 'robust', 'robust_per_batch'], is_ordered=False, sort_values=False), RangeParameter(name='layer2', parameter_type=INT, range=[32, 512]), RangeParameter(name='layer1', parameter_type=INT, range=[512, 1024]), RangeParameter(name='gamma', parameter_type=FLOAT, range=[1.0, 100.0], log_scale=True), RangeParameter(name='reg_entropy', parameter_type=FLOAT, range=[0.0001, 0.01], log_scale=True), RangeParameter(name='l1', parameter_type=FLOAT, range=[0.0001, 0.01], log_scale=True), RangeParameter(name='prune_threshold', parameter_type=FLOAT, range=[0.001, 0.003], log_scale=True)], parameter_constraints=[]).\n",
      "[INFO 05-13 21:01:57] ax.modelbridge.dispatch_utils: Using Bayesian optimization since there are more ordered parameters than there are categories for the unordered categorical parameters.\n",
      "[INFO 05-13 21:01:57] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=15 num_trials=None use_batch_trials=False\n",
      "[INFO 05-13 21:01:57] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=30\n",
      "[INFO 05-13 21:01:57] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=30\n",
      "[INFO 05-13 21:01:57] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 30 trials, GPEI for subsequent trials]). Iterations after 30 will take longer to generate due to model-fitting.\n",
      "[INFO 05-13 21:01:57] ax.service.managed_loop: Started full optimization with 100 steps.\n",
      "[INFO 05-13 21:01:57] ax.service.managed_loop: Running optimization trial 1...\n",
      "[WARNING 05-13 21:01:57] ax.utils.common.kwargs: `<class 'ax.models.random.sobol.SobolGenerator'>` expected argument `seed` to be of type typing.Union[int, NoneType]. Got 41 (type: <class 'int'>).\n",
      "[WARNING 05-13 21:01:57] ax.utils.common.kwargs: `<class 'ax.models.random.sobol.SobolGenerator'>` expected argument `deduplicate` to be of type <class 'bool'>. Got True (type: <class 'bool'>).\n",
      "[WARNING 05-13 21:01:57] ax.utils.common.kwargs: `<class 'ax.modelbridge.random.RandomModelBridge'>` expected argument `search_space` to be of type <class 'ax.core.search_space.SearchSpace'>. Got SearchSpace(parameters=[RangeParameter(name='nu', parameter_type=FLOAT, range=[0.0001, 100.0]), RangeParameter(name='lr', parameter_type=FLOAT, range=[0.0001, 0.01], log_scale=True), RangeParameter(name='wd', parameter_type=FLOAT, range=[1e-08, 1e-05], log_scale=True), RangeParameter(name='smoothing', parameter_type=FLOAT, range=[0.0, 0.2]), RangeParameter(name='margin', parameter_type=FLOAT, range=[0.0, 10.0]), RangeParameter(name='warmup', parameter_type=INT, range=[1, 1000]), RangeParameter(name='disc_b_warmup', parameter_type=INT, range=[1, 2]), RangeParameter(name='dropout', parameter_type=FLOAT, range=[0.0, 0.5]), ChoiceParameter(name='scaler', parameter_type=STRING, values=['standard_per_batch', 'standard', 'robust', 'robust_per_batch'], is_ordered=False, sort_values=False), RangeParameter(name='layer2', parameter_type=INT, range=[32, 512]), RangeParameter(name='layer1', parameter_type=INT, range=[512, 1024]), RangeParameter(name='gamma', parameter_type=FLOAT, range=[1.0, 100.0], log_scale=True), RangeParameter(name='reg_entropy', parameter_type=FLOAT, range=[0.0001, 0.01], log_scale=True), RangeParameter(name='l1', parameter_type=FLOAT, range=[0.0001, 0.01], log_scale=True), RangeParameter(name='prune_threshold', parameter_type=FLOAT, range=[0.001, 0.003], log_scale=True)], parameter_constraints=[]) (type: <class 'ax.core.search_space.SearchSpace'>).\n",
      "[WARNING 05-13 21:01:57] ax.utils.common.kwargs: `<class 'ax.modelbridge.random.RandomModelBridge'>` expected argument `experiment` to be of type typing.Union[ax.core.experiment.Experiment, NoneType]. Got Experiment(None) (type: <class 'ax.core.experiment.Experiment'>).\n",
      "[WARNING 05-13 21:01:57] ax.utils.common.kwargs: `<class 'ax.modelbridge.random.RandomModelBridge'>` expected argument `data` to be of type typing.Union[ax.core.data.Data, NoneType]. Got <ax.core.data.Data object at 0x7f41eba63370> (type: <class 'ax.core.data.Data'>).\n",
      "[WARNING 05-13 21:01:57] ax.utils.common.kwargs: `<class 'ax.modelbridge.random.RandomModelBridge'>` expected argument `optimization_config` to be of type typing.Union[ax.core.optimization_config.OptimizationConfig, NoneType]. Got None (type: <class 'NoneType'>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nu': 57.62209361696243, 'lr': 0.0002866874793641167, 'wd': 2.1707730937174506e-06, 'smoothing': 0, 'margin': 9.637901186943054, 'warmup': 363, 'disc_b_warmup': 1, 'dropout': 0.15570732951164246, 'layer2': 227, 'layer1': 956, 'gamma': 1.1582898620708149, 'reg_entropy': 0.005716725144156548, 'l1': 0.00115086353941168, 'prune_threshold': 0.001444340628738408, 'scaler': 'robust_per_batch', 'beta': 0, 'zeta': 0, 'thres': 0}\n",
      "See results using: tensorboard --logdir=logs/ae_then_classifier_holdout/4dc2c0e9-c467-469d-8cc3-10a902e3fff4 --port=6006\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/ADLab/BERNN/e/BE-434\n",
      "Rep: 0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: \n",
      "\u001b[95m\n",
      "----NeptuneEmptyLocationException----------------------------------------------\n",
      "\u001b[0m\n",
      "Neptune could not find files in the requested location (../data/subjects_experiment_ATN_verified_diagnosis.csv) during the creation of an Artifact in \"metadata\".\n",
      "\n",
      "\u001b[92mNeed help?\u001b[0m-> https://docs-legacy.neptune.ai/getting_help\n",
      "\n",
      "Best Loss Epoch 0, Losses: 0.7453204214572906, Domain Losses: 11.478661441802979, Domain Accuracy: 0.3234375\n",
      "Best Loss Epoch 1, Losses: 0.7074046432971954, Domain Losses: 11.788368129730225, Domain Accuracy: 0.30625\n",
      "Best Loss Epoch 2, Losses: 0.6738894820213318, Domain Losses: 11.2767840385437, Domain Accuracy: 0.3515625\n",
      "Best Loss Epoch 3, Losses: 0.6482175946235657, Domain Losses: 11.663066387176514, Domain Accuracy: 0.33125\n",
      "Best Loss Epoch 4, Losses: 0.6306762099266052, Domain Losses: 11.456602764129638, Domain Accuracy: 0.3234375\n",
      "Best Loss Epoch 5, Losses: 0.6182438969612122, Domain Losses: 11.251091003417969, Domain Accuracy: 0.2734375\n",
      "Best Loss Epoch 6, Losses: 0.6118470668792725, Domain Losses: 11.23715353012085, Domain Accuracy: 0.290625\n",
      "Best Loss Epoch 7, Losses: 0.5996949970722198, Domain Losses: 11.214899444580078, Domain Accuracy: 0.3046875\n",
      "Best Loss Epoch 8, Losses: 0.5866395115852356, Domain Losses: 11.364146995544434, Domain Accuracy: 0.303125\n",
      "Best Loss Epoch 9, Losses: 0.5819393813610076, Domain Losses: 11.285580158233643, Domain Accuracy: 0.3453125\n",
      "Best Loss Epoch 10, Losses: 0.5764338135719299, Domain Losses: 11.03913335800171, Domain Accuracy: 0.309375\n",
      "Best Loss Epoch 11, Losses: 0.5714317977428436, Domain Losses: 10.955471515655518, Domain Accuracy: 0.321875\n",
      "Best Loss Epoch 12, Losses: 0.5648282647132874, Domain Losses: 11.273216152191162, Domain Accuracy: 0.3203125\n",
      "Best Loss Epoch 13, Losses: 0.5609818935394287, Domain Losses: 11.220696926116943, Domain Accuracy: 0.2875\n",
      "Best Loss Epoch 14, Losses: 0.5559033870697021, Domain Losses: 10.989520931243897, Domain Accuracy: 0.33125\n",
      "Best Loss Epoch 15, Losses: 0.5498332262039185, Domain Losses: 11.161076831817628, Domain Accuracy: 0.2953125\n",
      "Best Loss Epoch 16, Losses: 0.5471001446247101, Domain Losses: 11.027960109710694, Domain Accuracy: 0.31875\n",
      "Best Loss Epoch 17, Losses: 0.543194031715393, Domain Losses: 11.00210199356079, Domain Accuracy: 0.3359375\n",
      "Best Loss Epoch 18, Losses: 0.5398338496685028, Domain Losses: 10.973638248443603, Domain Accuracy: 0.303125\n",
      "Best Loss Epoch 19, Losses: 0.5350001513957977, Domain Losses: 10.95616865158081, Domain Accuracy: 0.2859375\n",
      "Best Loss Epoch 20, Losses: 0.5336705327033997, Domain Losses: 11.060896110534667, Domain Accuracy: 0.3125\n",
      "Best Loss Epoch 21, Losses: 0.5306630253791809, Domain Losses: 11.102764987945557, Domain Accuracy: 0.30625\n",
      "Best Loss Epoch 22, Losses: 0.5286369323730469, Domain Losses: 11.272253704071044, Domain Accuracy: 0.321875\n",
      "Best Loss Epoch 23, Losses: 0.5259268105030059, Domain Losses: 11.153707027435303, Domain Accuracy: 0.3203125\n",
      "Best Loss Epoch 24, Losses: 0.5236418545246124, Domain Losses: 10.84359712600708, Domain Accuracy: 0.3375\n",
      "Best Loss Epoch 25, Losses: 0.522556659579277, Domain Losses: 10.961815357208252, Domain Accuracy: 0.346875\n",
      "Best Loss Epoch 26, Losses: 0.5218084245920181, Domain Losses: 10.882255172729492, Domain Accuracy: 0.33125\n",
      "Best Loss Epoch 27, Losses: 0.5194092273712159, Domain Losses: 10.981842994689941, Domain Accuracy: 0.325\n",
      "Best Loss Epoch 28, Losses: 0.5187345772981644, Domain Losses: 10.994178581237794, Domain Accuracy: 0.3375\n",
      "Best Loss Epoch 29, Losses: 0.5131258189678192, Domain Losses: 11.021944427490235, Domain Accuracy: 0.346875\n",
      "Best Loss Epoch 30, Losses: 0.5088563650846482, Domain Losses: 11.120267105102538, Domain Accuracy: 0.3125\n",
      "Best Loss Epoch 31, Losses: 0.5071736514568329, Domain Losses: 10.967926979064941, Domain Accuracy: 0.3234375\n",
      "Best Loss Epoch 32, Losses: 0.5050428688526154, Domain Losses: 10.720232200622558, Domain Accuracy: 0.34375\n",
      "Best Loss Epoch 33, Losses: 0.503576272726059, Domain Losses: 11.100150299072265, Domain Accuracy: 0.328125\n",
      "Best Loss Epoch 34, Losses: 0.5012796431779861, Domain Losses: 10.820446014404297, Domain Accuracy: 0.315625\n",
      "Best Loss Epoch 35, Losses: 0.5002439737319946, Domain Losses: 11.115310573577881, Domain Accuracy: 0.3171875\n",
      "Best Loss Epoch 36, Losses: 0.4966937184333801, Domain Losses: 11.082904815673828, Domain Accuracy: 0.3515625\n",
      "Best Loss Epoch 38, Losses: 0.495730996131897, Domain Losses: 10.95361270904541, Domain Accuracy: 0.3375\n",
      "Best Loss Epoch 40, Losses: 0.4932467728853226, Domain Losses: 10.771317481994629, Domain Accuracy: 0.3296875\n",
      "Best Loss Epoch 41, Losses: 0.4922077268362045, Domain Losses: 10.747257900238036, Domain Accuracy: 0.3140625\n",
      "Best Loss Epoch 42, Losses: 0.4907066971063614, Domain Losses: 11.014413356781006, Domain Accuracy: 0.3375\n",
      "Best Loss Epoch 43, Losses: 0.4900381535291672, Domain Losses: 10.882144165039062, Domain Accuracy: 0.3328125\n",
      "Best Loss Epoch 44, Losses: 0.4890255630016327, Domain Losses: 10.849477958679199, Domain Accuracy: 0.321875\n",
      "Best Loss Epoch 45, Losses: 0.48678968846797943, Domain Losses: 10.858239269256591, Domain Accuracy: 0.3265625\n",
      "Best Loss Epoch 46, Losses: 0.48418334424495696, Domain Losses: 11.06733226776123, Domain Accuracy: 0.3453125\n",
      "Best Loss Epoch 48, Losses: 0.48368494510650634, Domain Losses: 10.93040418624878, Domain Accuracy: 0.30625\n",
      "Best Loss Epoch 49, Losses: 0.4836388975381851, Domain Losses: 10.910592746734618, Domain Accuracy: 0.31875\n",
      "Best Loss Epoch 50, Losses: 0.4825993299484253, Domain Losses: 11.010249042510987, Domain Accuracy: 0.3234375\n",
      "Best Loss Epoch 51, Losses: 0.4808766424655914, Domain Losses: 10.915587997436523, Domain Accuracy: 0.3359375\n",
      "Best Loss Epoch 52, Losses: 0.47829297184944153, Domain Losses: 10.85629587173462, Domain Accuracy: 0.3203125\n",
      "Best Loss Epoch 54, Losses: 0.4756686359643936, Domain Losses: 11.202026748657227, Domain Accuracy: 0.3234375\n",
      "Best Loss Epoch 56, Losses: 0.4735962122678757, Domain Losses: 10.91029233932495, Domain Accuracy: 0.2890625\n",
      "Best Loss Epoch 57, Losses: 0.4728525310754776, Domain Losses: 10.843535137176513, Domain Accuracy: 0.3\n",
      "Best Loss Epoch 59, Losses: 0.4709054589271545, Domain Losses: 10.757380104064941, Domain Accuracy: 0.35625\n",
      "Best Loss Epoch 60, Losses: 0.4686905801296234, Domain Losses: 10.964823722839355, Domain Accuracy: 0.34375\n",
      "Best Loss Epoch 62, Losses: 0.46833967566490176, Domain Losses: 10.723756027221679, Domain Accuracy: 0.33125\n",
      "Best Loss Epoch 63, Losses: 0.46811549067497255, Domain Losses: 10.624483489990235, Domain Accuracy: 0.3125\n",
      "Best Loss Epoch 64, Losses: 0.4659159451723099, Domain Losses: 10.724927520751953, Domain Accuracy: 0.325\n",
      "Best Loss Epoch 65, Losses: 0.4655914306640625, Domain Losses: 10.85485029220581, Domain Accuracy: 0.328125\n",
      "Best Loss Epoch 66, Losses: 0.4651721179485321, Domain Losses: 10.690182113647461, Domain Accuracy: 0.3296875\n",
      "Best Loss Epoch 67, Losses: 0.4644451826810837, Domain Losses: 10.786817264556884, Domain Accuracy: 0.275\n",
      "Best Loss Epoch 71, Losses: 0.46326819956302645, Domain Losses: 10.694958686828613, Domain Accuracy: 0.325\n",
      "Best Loss Epoch 72, Losses: 0.4618665188550949, Domain Losses: 10.694026756286622, Domain Accuracy: 0.321875\n",
      "Best Loss Epoch 73, Losses: 0.46085694134235383, Domain Losses: 10.821642208099366, Domain Accuracy: 0.3578125\n",
      "Best Loss Epoch 74, Losses: 0.46064198911190035, Domain Losses: 10.843744087219239, Domain Accuracy: 0.3546875\n",
      "Best Loss Epoch 75, Losses: 0.45707477927207946, Domain Losses: 10.916397953033448, Domain Accuracy: 0.33125\n",
      "Best Loss Epoch 79, Losses: 0.45645278990268706, Domain Losses: 10.89465103149414, Domain Accuracy: 0.3484375\n",
      "Best Loss Epoch 81, Losses: 0.4555125296115875, Domain Losses: 10.741493511199952, Domain Accuracy: 0.3234375\n",
      "Best Loss Epoch 82, Losses: 0.4548818200826645, Domain Losses: 10.65412940979004, Domain Accuracy: 0.3671875\n",
      "Best Loss Epoch 83, Losses: 0.4546066254377365, Domain Losses: 10.613254070281982, Domain Accuracy: 0.3578125\n",
      "Best Loss Epoch 84, Losses: 0.4526424676179886, Domain Losses: 10.661541748046876, Domain Accuracy: 0.359375\n",
      "Best Loss Epoch 85, Losses: 0.4524299740791321, Domain Losses: 10.681256771087646, Domain Accuracy: 0.3671875\n",
      "Best Loss Epoch 88, Losses: 0.45151967406272886, Domain Losses: 10.631731224060058, Domain Accuracy: 0.315625\n",
      "Best Loss Epoch 89, Losses: 0.4511419773101807, Domain Losses: 10.61121072769165, Domain Accuracy: 0.353125\n",
      "Best Loss Epoch 93, Losses: 0.4482275426387787, Domain Losses: 10.689929103851318, Domain Accuracy: 0.321875\n",
      "Best Loss Epoch 96, Losses: 0.44587846100330353, Domain Losses: 10.654829978942871, Domain Accuracy: 0.3859375\n",
      "Best Loss Epoch 98, Losses: 0.44462880194187165, Domain Losses: 10.647646427154541, Domain Accuracy: 0.35\n",
      "Best Loss Epoch 99, Losses: 0.4435404449701309, Domain Losses: 10.680073738098145, Domain Accuracy: 0.353125\n",
      "Best Loss Epoch 100, Losses: 0.44324075281620023, Domain Losses: 10.684830379486083, Domain Accuracy: 0.3625\n",
      "Best Loss Epoch 102, Losses: 0.44236046373844146, Domain Losses: 10.714228630065918, Domain Accuracy: 0.3359375\n",
      "Best Loss Epoch 104, Losses: 0.4409560799598694, Domain Losses: 10.649152755737305, Domain Accuracy: 0.3484375\n",
      "Best Loss Epoch 105, Losses: 0.440713432431221, Domain Losses: 10.497422313690185, Domain Accuracy: 0.321875\n",
      "Best Loss Epoch 106, Losses: 0.4380476176738739, Domain Losses: 10.685823440551758, Domain Accuracy: 0.3390625\n",
      "Best Loss Epoch 109, Losses: 0.4372190535068512, Domain Losses: 10.606287288665772, Domain Accuracy: 0.328125\n",
      "Best Loss Epoch 111, Losses: 0.4366870880126953, Domain Losses: 10.722816562652588, Domain Accuracy: 0.3171875\n",
      "Best Loss Epoch 112, Losses: 0.4359081000089645, Domain Losses: 10.630877208709716, Domain Accuracy: 0.3609375\n",
      "Best Loss Epoch 116, Losses: 0.4348668932914734, Domain Losses: 10.569507312774657, Domain Accuracy: 0.321875\n",
      "Best Loss Epoch 117, Losses: 0.43206401765346525, Domain Losses: 10.53563413619995, Domain Accuracy: 0.3515625\n",
      "Best Loss Epoch 118, Losses: 0.4316455483436584, Domain Losses: 10.734606075286866, Domain Accuracy: 0.34375\n",
      "Best Loss Epoch 119, Losses: 0.4297871619462967, Domain Losses: 10.64597806930542, Domain Accuracy: 0.3625\n",
      "Best Loss Epoch 121, Losses: 0.42977885603904725, Domain Losses: 10.797003269195557, Domain Accuracy: 0.33125\n",
      "Best Loss Epoch 123, Losses: 0.42883644700050355, Domain Losses: 10.638769149780273, Domain Accuracy: 0.353125\n",
      "Best Loss Epoch 124, Losses: 0.42711669504642485, Domain Losses: 10.603175258636474, Domain Accuracy: 0.365625\n",
      "Best Loss Epoch 125, Losses: 0.42458040416240694, Domain Losses: 10.568112182617188, Domain Accuracy: 0.3203125\n",
      "Best Loss Epoch 130, Losses: 0.42344211637973783, Domain Losses: 10.547086143493653, Domain Accuracy: 0.3234375\n",
      "Best Loss Epoch 133, Losses: 0.42320782542228697, Domain Losses: 10.71212043762207, Domain Accuracy: 0.3203125\n",
      "Best Loss Epoch 134, Losses: 0.42084912955760956, Domain Losses: 10.545919132232665, Domain Accuracy: 0.3265625\n",
      "Best Loss Epoch 139, Losses: 0.42039133310317994, Domain Losses: 10.65227289199829, Domain Accuracy: 0.303125\n",
      "Best Loss Epoch 141, Losses: 0.4203276127576828, Domain Losses: 10.601030254364014, Domain Accuracy: 0.3453125\n",
      "Best Loss Epoch 142, Losses: 0.4194289267063141, Domain Losses: 10.649696826934814, Domain Accuracy: 0.31875\n",
      "Best Loss Epoch 143, Losses: 0.41742804944515227, Domain Losses: 10.63030185699463, Domain Accuracy: 0.3265625\n",
      "Best Loss Epoch 146, Losses: 0.4170746296644211, Domain Losses: 10.429726028442383, Domain Accuracy: 0.3421875\n",
      "Best Loss Epoch 149, Losses: 0.41538929343223574, Domain Losses: 10.451778221130372, Domain Accuracy: 0.3328125\n"
     ]
    }
   ],
   "source": [
    "train = TrainAEThenClassifierHoldout(args, args.path, fix_thres=-1, load_tb=False, \n",
    "                                        log_metrics=1, keep_models=False,\n",
    "                                        log_inputs=False, log_plots=1,\n",
    "                                        log_tb=False, log_neptune=True, log_mlflow=True, \n",
    "                                        groupkfold=1, pools=True)\n",
    "\n",
    "# train.train()\n",
    "# List of hyperparameters getting optimized\n",
    "parameters = [\n",
    "    {\"name\": \"nu\", \"type\": \"range\", \"bounds\": [1e-4, 1e2], \"log_scale\": False},\n",
    "    {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-4, 1e-2], \"log_scale\": True},\n",
    "    {\"name\": \"wd\", \"type\": \"range\", \"bounds\": [1e-8, 1e-5], \"log_scale\": True},\n",
    "    {\"name\": \"smoothing\", \"type\": \"range\", \"bounds\": [0., 0.2]},\n",
    "    {\"name\": \"margin\", \"type\": \"range\", \"bounds\": [0., 10.]},\n",
    "    {\"name\": \"warmup\", \"type\": \"range\", \"bounds\": [1, 10]},\n",
    "    {\"name\": \"disc_b_warmup\", \"type\": \"range\", \"bounds\": [1, 2]},\n",
    "    {\"name\": \"dropout\", \"type\": \"range\", \"bounds\": [0.0, 0.5]},\n",
    "    {\"name\": \"scaler\", \"type\": \"choice\",\n",
    "        \"values\": ['standard_per_batch', 'standard', 'robust', 'robust_per_batch']},  # scaler whould be no for zinb\n",
    "    {\"name\": \"layer2\", \"type\": \"range\", \"bounds\": [32, 512]},\n",
    "    {\"name\": \"layer1\", \"type\": \"range\", \"bounds\": [512, 1024]},\n",
    "    \n",
    "]\n",
    "\n",
    "# Some hyperparameters are not always required. They are set to a default value in Train.train()\n",
    "if args.dloss in ['revTriplet', 'revDANN', 'DANN', 'inverseTriplet', 'normae']:\n",
    "    # gamma = 0 will ensure DANN is not learned\n",
    "    parameters += [{\"name\": \"gamma\", \"type\": \"range\", \"bounds\": [1e-0, 1e2], \"log_scale\": True}]\n",
    "if args.variational:\n",
    "    # beta = 0 because useless outside a variational autoencoder\n",
    "    parameters += [{\"name\": \"beta\", \"type\": \"range\", \"bounds\": [1e-2, 1e2], \"log_scale\": True}]\n",
    "if args.zinb:\n",
    "    # zeta = 0 because useless outside a zinb autoencoder\n",
    "    parameters += [{\"name\": \"zeta\", \"type\": \"range\", \"bounds\": [1e-2, 1e2], \"log_scale\": True}]\n",
    "if args.kan and args.use_l1:\n",
    "    # zeta = 0 because useless outside a zinb autoencoder\n",
    "    parameters += [{\"name\": \"reg_entropy\", \"type\": \"range\", \"bounds\": [1e-4, 1e-2], \"log_scale\": True}]\n",
    "if args.use_l1:\n",
    "    parameters += [{\"name\": \"l1\", \"type\": \"range\", \"bounds\": [1e-4, 1e-2], \"log_scale\": True}]\n",
    "if args.prune_network:\n",
    "    parameters += [{\"name\": \"prune_threshold\", \"type\": \"range\", \"bounds\": [1e-3, 3e-3], \"log_scale\": True}]\n",
    "\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=parameters,\n",
    "    evaluation_function=train.train,\n",
    "    objective_name='mcc',\n",
    "    minimize=False,\n",
    "    total_trials=args.n_trials,\n",
    "    random_seed=41,\n",
    "\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
