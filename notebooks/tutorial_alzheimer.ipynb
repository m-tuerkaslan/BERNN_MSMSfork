{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be436aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bernn import TrainAEThenClassifierHoldout\n",
    "from ax.service.managed_loop import optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccfe0a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace argparse with a simple class to simulate arguments\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.random_recs = 0  # TODO to deprecate, no longer used\n",
    "        self.predict_tests = 0\n",
    "        # self.balanced_rec_loader = 0\n",
    "        self.early_stop = 50\n",
    "        self.early_warmup_stop = -1\n",
    "        self.train_after_warmup = 0\n",
    "        self.threshold = 0.0\n",
    "        self.n_epochs = 1000\n",
    "        self.n_trials = 100\n",
    "        self.device = 'cuda:0'\n",
    "        self.rec_loss = 'l1'\n",
    "        self.tied_weights = 0\n",
    "        self.random = 1\n",
    "        self.variational = 0\n",
    "        self.zinb = 0  # TODO resolve problems, do not use\n",
    "        self.use_mapping = 1  # Use batch mapping for reconstruct\n",
    "        self.bdisc = 1\n",
    "        self.n_repeats = 5\n",
    "        self.dloss = 'inverseTriplet'  # one of revDANN, DANN, inverseTriplet, revTriplet\n",
    "        self.csv_file = 'unique_genes.csv'\n",
    "        self.best_features_file = ''  # best_unique_genes.tsv\n",
    "        self.bad_batches = ''  # 0;23;22;21;20;19;18;17;16;15\n",
    "        self.remove_zeros = 0\n",
    "        self.n_meta = 0\n",
    "        self.embeddings_meta = 0\n",
    "        self.features_to_keep = 'features_proteins.csv'\n",
    "        self.groupkfold = 1\n",
    "        self.dataset = 'alzheimer'\n",
    "        self.bs = 32  # Batch size\n",
    "        self.path = '../data/Alzheimer/'\n",
    "        self.exp_id = 'default_ae_then_classifier'\n",
    "        self.strategy = 'CU_DEM'  # only for Alzheimer dataset\n",
    "        self.n_agg = 5  # Number of trailing values to get stable valid values\n",
    "        self.n_layers = 2  # N layers for classifier\n",
    "        self.log1p = 1  # log1p the data? Should be 0 with zinb\n",
    "        self.pool = 1  # only for Alzheimer dataset\n",
    "        self.kan = 1\n",
    "        self.update_grid = 1\n",
    "        self.use_l1 = 1\n",
    "        self.clip_val = 1.0\n",
    "        self.log_metrics = 1\n",
    "        self.log_plots = 1\n",
    "        self.prune_network = 1.0\n",
    "\n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f952a638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 05-13 16:40:11] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter nu. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 16:40:11] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter lr. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 16:40:11] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter wd. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 16:40:11] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter smoothing. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 16:40:11] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter margin. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 16:40:11] ax.service.utils.instantiation: Inferred value type of ParameterType.INT for parameter warmup. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 16:40:11] ax.service.utils.instantiation: Inferred value type of ParameterType.INT for parameter disc_b_warmup. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 16:40:11] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter dropout. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 16:40:11] ax.service.utils.instantiation: Inferred value type of ParameterType.STRING for parameter scaler. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 16:40:11] ax.service.utils.instantiation: Inferred value type of ParameterType.INT for parameter layer2. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 16:40:11] ax.service.utils.instantiation: Inferred value type of ParameterType.INT for parameter layer1. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 16:40:11] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter gamma. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 16:40:11] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter reg_entropy. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 16:40:11] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter l1. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 16:40:11] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter prune_threshold. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 05-13 16:40:11] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='nu', parameter_type=FLOAT, range=[0.0001, 100.0]), RangeParameter(name='lr', parameter_type=FLOAT, range=[0.0001, 0.01], log_scale=True), RangeParameter(name='wd', parameter_type=FLOAT, range=[1e-08, 1e-05], log_scale=True), RangeParameter(name='smoothing', parameter_type=FLOAT, range=[0.0, 0.2]), RangeParameter(name='margin', parameter_type=FLOAT, range=[0.0, 10.0]), RangeParameter(name='warmup', parameter_type=INT, range=[1, 1000]), RangeParameter(name='disc_b_warmup', parameter_type=INT, range=[1, 2]), RangeParameter(name='dropout', parameter_type=FLOAT, range=[0.0, 0.5]), ChoiceParameter(name='scaler', parameter_type=STRING, values=['standard_per_batch', 'standard', 'robust', 'robust_per_batch'], is_ordered=False, sort_values=False), RangeParameter(name='layer2', parameter_type=INT, range=[32, 512]), RangeParameter(name='layer1', parameter_type=INT, range=[512, 1024]), RangeParameter(name='gamma', parameter_type=FLOAT, range=[0.01, 100.0], log_scale=True), RangeParameter(name='reg_entropy', parameter_type=FLOAT, range=[0.0001, 0.01], log_scale=True), RangeParameter(name='l1', parameter_type=FLOAT, range=[0.0001, 0.01], log_scale=True), RangeParameter(name='prune_threshold', parameter_type=FLOAT, range=[0.001, 0.003], log_scale=True)], parameter_constraints=[]).\n",
      "[INFO 05-13 16:40:11] ax.modelbridge.dispatch_utils: Using Bayesian optimization since there are more ordered parameters than there are categories for the unordered categorical parameters.\n",
      "[INFO 05-13 16:40:11] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=15 num_trials=None use_batch_trials=False\n",
      "[INFO 05-13 16:40:11] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=30\n",
      "[INFO 05-13 16:40:11] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=30\n",
      "[INFO 05-13 16:40:11] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 30 trials, GPEI for subsequent trials]). Iterations after 30 will take longer to generate due to model-fitting.\n",
      "[INFO 05-13 16:40:11] ax.service.managed_loop: Started full optimization with 100 steps.\n",
      "[INFO 05-13 16:40:11] ax.service.managed_loop: Running optimization trial 1...\n",
      "[WARNING 05-13 16:40:11] ax.utils.common.kwargs: `<class 'ax.models.random.sobol.SobolGenerator'>` expected argument `seed` to be of type typing.Union[int, NoneType]. Got 41 (type: <class 'int'>).\n",
      "[WARNING 05-13 16:40:11] ax.utils.common.kwargs: `<class 'ax.models.random.sobol.SobolGenerator'>` expected argument `deduplicate` to be of type <class 'bool'>. Got True (type: <class 'bool'>).\n",
      "[WARNING 05-13 16:40:11] ax.utils.common.kwargs: `<class 'ax.modelbridge.random.RandomModelBridge'>` expected argument `search_space` to be of type <class 'ax.core.search_space.SearchSpace'>. Got SearchSpace(parameters=[RangeParameter(name='nu', parameter_type=FLOAT, range=[0.0001, 100.0]), RangeParameter(name='lr', parameter_type=FLOAT, range=[0.0001, 0.01], log_scale=True), RangeParameter(name='wd', parameter_type=FLOAT, range=[1e-08, 1e-05], log_scale=True), RangeParameter(name='smoothing', parameter_type=FLOAT, range=[0.0, 0.2]), RangeParameter(name='margin', parameter_type=FLOAT, range=[0.0, 10.0]), RangeParameter(name='warmup', parameter_type=INT, range=[1, 1000]), RangeParameter(name='disc_b_warmup', parameter_type=INT, range=[1, 2]), RangeParameter(name='dropout', parameter_type=FLOAT, range=[0.0, 0.5]), ChoiceParameter(name='scaler', parameter_type=STRING, values=['standard_per_batch', 'standard', 'robust', 'robust_per_batch'], is_ordered=False, sort_values=False), RangeParameter(name='layer2', parameter_type=INT, range=[32, 512]), RangeParameter(name='layer1', parameter_type=INT, range=[512, 1024]), RangeParameter(name='gamma', parameter_type=FLOAT, range=[0.01, 100.0], log_scale=True), RangeParameter(name='reg_entropy', parameter_type=FLOAT, range=[0.0001, 0.01], log_scale=True), RangeParameter(name='l1', parameter_type=FLOAT, range=[0.0001, 0.01], log_scale=True), RangeParameter(name='prune_threshold', parameter_type=FLOAT, range=[0.001, 0.003], log_scale=True)], parameter_constraints=[]) (type: <class 'ax.core.search_space.SearchSpace'>).\n",
      "[WARNING 05-13 16:40:11] ax.utils.common.kwargs: `<class 'ax.modelbridge.random.RandomModelBridge'>` expected argument `experiment` to be of type typing.Union[ax.core.experiment.Experiment, NoneType]. Got Experiment(None) (type: <class 'ax.core.experiment.Experiment'>).\n",
      "[WARNING 05-13 16:40:11] ax.utils.common.kwargs: `<class 'ax.modelbridge.random.RandomModelBridge'>` expected argument `data` to be of type typing.Union[ax.core.data.Data, NoneType]. Got <ax.core.data.Data object at 0x7f918bc521c0> (type: <class 'ax.core.data.Data'>).\n",
      "[WARNING 05-13 16:40:11] ax.utils.common.kwargs: `<class 'ax.modelbridge.random.RandomModelBridge'>` expected argument `optimization_config` to be of type typing.Union[ax.core.optimization_config.OptimizationConfig, NoneType]. Got None (type: <class 'NoneType'>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nu': 57.62209361696243, 'lr': 0.0002866874793641167, 'wd': 2.1707730937174506e-06, 'smoothing': 0, 'margin': 9.637901186943054, 'warmup': 363, 'disc_b_warmup': 1, 'dropout': 0.15570732951164246, 'layer2': 227, 'layer1': 956, 'gamma': 0.013416354045760274, 'reg_entropy': 0.005716725144156548, 'l1': 0.00115086353941168, 'prune_threshold': 0.001444340628738408, 'scaler': 'robust_per_batch', 'beta': 0, 'zeta': 0, 'thres': 0}\n",
      "See results using: tensorboard --logdir=logs/ae_then_classifier_holdout/c5daecba-b7e6-43d8-9057-5ca1ae38ea75 --port=6006\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/ADLab/BERNN/e/BE-420\n",
      "Rep: 0\n",
      "Best Loss Epoch 0, Losses: 0.9252324146883828, Domain Losses: 13.952556814466204, Domain Accuracy: 0.05133928571428571\n",
      "Best Loss Epoch 1, Losses: 0.8122090995311737, Domain Losses: 13.686608927590507, Domain Accuracy: 0.05915178571428571\n",
      "Best Loss Epoch 2, Losses: 0.7699982055595943, Domain Losses: 13.280011177062988, Domain Accuracy: 0.04799107142857143\n",
      "Best Loss Epoch 3, Losses: 0.7388248060430799, Domain Losses: 13.001364503587995, Domain Accuracy: 0.046875\n",
      "Best Loss Epoch 4, Losses: 0.7207536101341248, Domain Losses: 12.386845179966517, Domain Accuracy: 0.04575892857142857\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mprune_network:\n\u001b[1;32m     49\u001b[0m     parameters \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprune_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrange\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbounds\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m1e-3\u001b[39m, \u001b[38;5;241m3e-3\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}]\n\u001b[0;32m---> 51\u001b[0m best_parameters, values, experiment, model \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmcc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m41\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ax/service/managed_loop.py:296\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(parameters, evaluation_function, experiment_name, objective_name, minimize, parameter_constraints, outcome_constraints, total_trials, arms_per_trial, random_seed, generation_strategy)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct and run a full optimization loop.\"\"\"\u001b[39;00m\n\u001b[1;32m    283\u001b[0m loop \u001b[38;5;241m=\u001b[39m OptimizationLoop\u001b[38;5;241m.\u001b[39mwith_evaluation_function(\n\u001b[1;32m    284\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    285\u001b[0m     objective_name\u001b[38;5;241m=\u001b[39mobjective_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m     generation_strategy\u001b[38;5;241m=\u001b[39mgeneration_strategy,\n\u001b[1;32m    295\u001b[0m )\n\u001b[0;32m--> 296\u001b[0m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m parameterization, values \u001b[38;5;241m=\u001b[39m loop\u001b[38;5;241m.\u001b[39mget_best_point()\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parameterization, values, loop\u001b[38;5;241m.\u001b[39mexperiment, loop\u001b[38;5;241m.\u001b[39mget_current_model()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ax/service/managed_loop.py:227\u001b[0m, in \u001b[0;36mOptimizationLoop.full_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps):\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 227\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m SearchSpaceExhausted \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    229\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    230\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped optimization as the search space is exhaused. Message \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom generation strategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ax/utils/common/executils.py:161\u001b[0m, in \u001b[0;36mretry_on_exception.<locals>.func_wrapper.<locals>.actual_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m             wait_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\n\u001b[1;32m    158\u001b[0m                 MAX_WAIT_SECONDS, initial_wait_seconds \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    159\u001b[0m             )\n\u001b[1;32m    160\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(wait_interval)\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# If we are here, it means the retries were finished but\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# The error was suppressed. Hence return the default value provided.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m default_return_on_suppression\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ax/service/managed_loop.py:205\u001b[0m, in \u001b[0;36mOptimizationLoop.run_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m trial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_new_trial()\n\u001b[1;32m    203\u001b[0m trial\u001b[38;5;241m.\u001b[39mmark_running(no_runner_required\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    204\u001b[0m _, data \u001b[38;5;241m=\u001b[39m InstantiationBase\u001b[38;5;241m.\u001b[39mdata_and_evaluations_from_raw_data(\n\u001b[0;32m--> 205\u001b[0m     raw_data\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    206\u001b[0m         arm\u001b[38;5;241m.\u001b[39mname: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_evaluation_function(arm\u001b[38;5;241m.\u001b[39mparameters, weight)\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m arm, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_weights_by_arm(trial)\n\u001b[1;32m    208\u001b[0m     },\n\u001b[1;32m    209\u001b[0m     trial_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_trial,\n\u001b[1;32m    210\u001b[0m     sample_sizes\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    211\u001b[0m     metric_names\u001b[38;5;241m=\u001b[39mnot_none(\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39moptimization_config\n\u001b[1;32m    213\u001b[0m     )\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mmetric_names,\n\u001b[1;32m    214\u001b[0m )\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mattach_data(data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m    217\u001b[0m trial\u001b[38;5;241m.\u001b[39mmark_completed()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ax/service/managed_loop.py:206\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    201\u001b[0m trial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_new_trial()\n\u001b[1;32m    203\u001b[0m trial\u001b[38;5;241m.\u001b[39mmark_running(no_runner_required\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    204\u001b[0m _, data \u001b[38;5;241m=\u001b[39m InstantiationBase\u001b[38;5;241m.\u001b[39mdata_and_evaluations_from_raw_data(\n\u001b[1;32m    205\u001b[0m     raw_data\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m--> 206\u001b[0m         arm\u001b[38;5;241m.\u001b[39mname: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_evaluation_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43marm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m arm, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_weights_by_arm(trial)\n\u001b[1;32m    208\u001b[0m     },\n\u001b[1;32m    209\u001b[0m     trial_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_trial,\n\u001b[1;32m    210\u001b[0m     sample_sizes\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    211\u001b[0m     metric_names\u001b[38;5;241m=\u001b[39mnot_none(\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39moptimization_config\n\u001b[1;32m    213\u001b[0m     )\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mmetric_names,\n\u001b[1;32m    214\u001b[0m )\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mattach_data(data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m    217\u001b[0m trial\u001b[38;5;241m.\u001b[39mmark_completed()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ax/service/managed_loop.py:145\u001b[0m, in \u001b[0;36mOptimizationLoop._call_evaluation_function\u001b[0;34m(self, parameterization, weight)\u001b[0m\n\u001b[1;32m    142\u001b[0m num_evaluation_function_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_evaluation_function_params \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# pyre-fixme[20]: Anonymous call expects argument `$1`.\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     evaluation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameterization\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m num_evaluation_function_params \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    147\u001b[0m     evaluation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_function(parameterization, weight)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/bernn/dl/train/train_ae_then_classifier_holdout.py:567\u001b[0m, in \u001b[0;36mTrainAEThenClassifierHoldout.train\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    565\u001b[0m     dom_acc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(traces[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdom_acc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m warmup:\n\u001b[0;32m--> 567\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete_log_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/warmup.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    570\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mearly_warmup_stop \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m warmup_counter \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mearly_warmup_stop) \u001b[38;5;129;01mand\u001b[39;00m warmup:  \u001b[38;5;66;03m# or warmup_counter == 100:\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# When the warnup counter gets to\u001b[39;00m\n\u001b[1;32m    572\u001b[0m     values \u001b[38;5;241m=\u001b[39m log_traces(traces, values)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/serialization.py:442\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m--> 442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/serialization.py:291\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = TrainAEThenClassifierHoldout(args, args.path, fix_thres=-1, load_tb=False, \n",
    "                                        log_metrics=args.log_metrics, keep_models=False,\n",
    "                                        log_inputs=False, log_plots=args.log_plots,\n",
    "                                        log_tb=False, log_neptune=True, log_mlflow=True, \n",
    "                                        groupkfold=args.groupkfold, pools=True)\n",
    "\n",
    "# train.train()\n",
    "# List of hyperparameters getting optimized\n",
    "parameters = [\n",
    "    {\"name\": \"nu\", \"type\": \"range\", \"bounds\": [1e-4, 1e2], \"log_scale\": False},\n",
    "    {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-4, 1e-2], \"log_scale\": True},\n",
    "    {\"name\": \"wd\", \"type\": \"range\", \"bounds\": [1e-8, 1e-5], \"log_scale\": True},\n",
    "    # {\"name\": \"l1\", \"type\": \"range\", \"bounds\": [1e-8, 1e-5], \"log_scale\": True},\n",
    "    # {\"name\": \"lr_b\", \"type\": \"range\", \"bounds\": [1e-6, 1e-1], \"log_scale\": True},\n",
    "    # {\"name\": \"wd_b\", \"type\": \"range\", \"bounds\": [1e-8, 1e-5], \"log_scale\": True},\n",
    "    {\"name\": \"smoothing\", \"type\": \"range\", \"bounds\": [0., 0.2]},\n",
    "    {\"name\": \"margin\", \"type\": \"range\", \"bounds\": [0., 10.]},\n",
    "    {\"name\": \"warmup\", \"type\": \"range\", \"bounds\": [1, 1000]},\n",
    "    {\"name\": \"disc_b_warmup\", \"type\": \"range\", \"bounds\": [1, 2]},\n",
    "\n",
    "    {\"name\": \"dropout\", \"type\": \"range\", \"bounds\": [0.0, 0.5]},\n",
    "    # {\"name\": \"ncols\", \"type\": \"range\", \"bounds\": [20, 10000]},\n",
    "    {\"name\": \"scaler\", \"type\": \"choice\",\n",
    "        \"values\": ['standard_per_batch', 'standard', 'robust', 'robust_per_batch']},  # scaler whould be no for zinb\n",
    "    # {\"name\": \"layer3\", \"type\": \"range\", \"bounds\": [32, 512]},\n",
    "    {\"name\": \"layer2\", \"type\": \"range\", \"bounds\": [32, 512]},\n",
    "    {\"name\": \"layer1\", \"type\": \"range\", \"bounds\": [512, 1024]},\n",
    "    # {\"name\": \"layer2\", \"type\": \"range\", \"bounds\": [32, 64]},\n",
    "    # {\"name\": \"layer1\", \"type\": \"range\", \"bounds\": [64, 128]},\n",
    "    \n",
    "]\n",
    "\n",
    "# Some hyperparameters are not always required. They are set to a default value in Train.train()\n",
    "if args.dloss in ['revTriplet', 'revDANN', 'DANN', 'inverseTriplet', 'normae']:\n",
    "    # gamma = 0 will ensure DANN is not learned\n",
    "    parameters += [{\"name\": \"gamma\", \"type\": \"range\", \"bounds\": [1e-2, 1e2], \"log_scale\": True}]\n",
    "if args.variational:\n",
    "    # beta = 0 because useless outside a variational autoencoder\n",
    "    parameters += [{\"name\": \"beta\", \"type\": \"range\", \"bounds\": [1e-2, 1e2], \"log_scale\": True}]\n",
    "if args.zinb:\n",
    "    # zeta = 0 because useless outside a zinb autoencoder\n",
    "    parameters += [{\"name\": \"zeta\", \"type\": \"range\", \"bounds\": [1e-2, 1e2], \"log_scale\": True}]\n",
    "if args.kan and args.use_l1:\n",
    "    # zeta = 0 because useless outside a zinb autoencoder\n",
    "    parameters += [{\"name\": \"reg_entropy\", \"type\": \"range\", \"bounds\": [1e-4, 1e-2], \"log_scale\": True}]\n",
    "if args.use_l1:\n",
    "    parameters += [{\"name\": \"l1\", \"type\": \"range\", \"bounds\": [1e-4, 1e-2], \"log_scale\": True}]\n",
    "if args.prune_network:\n",
    "    parameters += [{\"name\": \"prune_threshold\", \"type\": \"range\", \"bounds\": [1e-3, 3e-3], \"log_scale\": True}]\n",
    "\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=parameters,\n",
    "    evaluation_function=train.train,\n",
    "    objective_name='mcc',\n",
    "    minimize=False,\n",
    "    total_trials=args.n_trials,\n",
    "    random_seed=41,\n",
    "\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
